import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation, PillowWriter
dados = pd.read_csv('artificial1d.csv', header=None, names=['x', 'y'])
print("Primeiras linhas dos dados:")
print(dados.head())
x = dados['x'].values
y = dados['y'].values
learning_rate = 0.01
epochs = 100
def ols_regression(x, y):
    X = np.vstack([np.ones(len(x)), x]).T
    theta_ols = np.linalg.inv(X.T @ X) @ X.T @ y
    y_pred = X @ theta_ols
    mse_ols = np.mean((y - y_pred) ** 2)
    return theta_ols, mse_ols, y_pred
def gradient_descent(x, y, learning_rate, epochs):
    w0, w1 = 0, 0
    mse_list = []
    w0_history, w1_history = [], []
    for epoch in range(epochs):
        y_pred = w0 + w1 * x
        error = y - y_pred
        mse = np.mean(error ** 2)
        mse_list.append(mse)
        
        w0 -= learning_rate * (-2 * error.mean())
        w1 -= learning_rate * (-2 * (error * x).mean())
        
        w0_history.append(w0)
        w1_history.append(w1)
    return w0, w1, mse_list, w0_history, w1_history

def stochastic_gradient_descent(x, y, learning_rate, epochs):
    w0, w1 = 0, 0
    mse_list = []
    w0_history, w1_history = [], []
    for epoch in range(epochs):
        mse_epoch = 0
        for i in range(len(x)):
            y_pred_i = w0 + w1 * x[i]
            error_i = y[i] - y_pred_i
            mse_epoch += error_i ** 2
            
            w0 += learning_rate * 2 * error_i
            w1 += learning_rate * 2 * error_i * x[i]
        
        mse_list.append(mse_epoch / len(x))
        w0_history.append(w0)
        w1_history.append(w1)
    return w0, w1, mse_list, w0_history, w1_history
theta_ols, mse_ols, y_pred_ols = ols_regression(x, y)
w0_gd, w1_gd, mse_list_gd, w0_history_gd, w1_history_gd = gradient_descent(x, y, learning_rate, epochs)
w0_sgd, w1_sgd, mse_list_sgd, w0_history_sgd, w1_history_sgd = stochastic_gradient_descent(x, y, learning_rate, epochs)
def animate_gd(epoch):
    plt.cla()
    plt.scatter(x, y, color="blue", label="Dados")
    y_pred = w0_history_gd[epoch] + w1_history_gd[epoch] * x
    plt.plot(x, y_pred, color="green", label=f"GD Fit (Epoch {epoch + 1})")
    plt.title("Gradiente Descendente")
    plt.legend()
def animate_sgd(epoch):
    plt.cla()
    plt.scatter(x, y, color="blue", label="Dados")
    y_pred = w0_history_sgd[epoch] + w1_history_sgd[epoch] * x
    plt.plot(x, y_pred, color="purple", label=f"SGD Fit (Epoch {epoch + 1})")
    plt.title("Gradiente Descendente Estocástico")
    plt.legend()
fig_gd = plt.figure()
anim_gd = FuncAnimation(fig_gd, animate_gd, frames=epochs, repeat=False)
anim_gd.save("gradiente_descendente.gif", writer=PillowWriter(fps=5))
fig_sgd = plt.figure()
anim_sgd = FuncAnimation(fig_sgd, animate_sgd, frames=epochs, repeat=False)
anim_sgd.save("gradiente_descendente_estocastico.gif", writer=PillowWriter(fps=5))
print("Parâmetros (OLS):", theta_ols)
print("MSE (OLS):", mse_ols)
print("Parâmetros (GD):", (w0_gd, w1_gd))
print("MSE (GD):", mse_list_gd[-1])
print("Parâmetros (SGD):", (w0_sgd, w1_sgd))
print("MSE (SGD):", mse_list_sgd[-1])
plt.figure()
plt.scatter(x, y, color="blue", label="Dados")
plt.plot(x, y_pred_ols, color="red", label="OLS Fit")
plt.legend()
plt.title("Regressão Linear (OLS)")
plt.show()
plt.figure()
plt.plot(mse_list_gd, label="GD MSE")
plt.plot(mse_list_sgd, label="SGD MSE", linestyle="--")
plt.xlabel("Épocas")
plt.ylabel("MSE")
plt.legend()
plt.title("Curva de Aprendizagem")
plt.show()
